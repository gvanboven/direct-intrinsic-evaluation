{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import codecs\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = {\n",
    "    \"n2v_additive\" : \"..\\Model_Results\\\\Additive_Refined.csv\",\n",
    "    \"n2v_standard\" : \"..\\Model_Results\\\\N2V_consistency_refined.csv\",\n",
    "    \"n2v_consistency\" : \"..\\Model_Results\\\\N2V_standard_refined.csv\",\n",
    "    \"w2v\" : \"..\\Model_Results\\\\w2v_all_results_refined.csv\",\n",
    "    \"hyper_all\" : \"..\\Model_Results\\\\Hyperword_results_Quine_wiki_processed_Refined.csv\",\n",
    "    \"hyper_quine\" : \"..\\Model_Results\\\\Hyperword_results_quine_processed_Refined.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_list(file, dict_name):\n",
    "    with open(file, 'r', encoding=\"utf8\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for n, term, nn1, nn2, nn3, nn5, nn10, nn50, outlier in reader:\n",
    "            if term == \"term\":\n",
    "                continue\n",
    "            dict_name[term] = {\"nn1\" : nn1,\n",
    "                              \"nn2\" : nn2,\n",
    "                               \"nn3\": nn3,\n",
    "                               \"nn5\" : nn5,\n",
    "                               \"nn10\" : nn10,\n",
    "                               \"nn50\" : nn50,\n",
    "                               \"outlier\" : outlier\n",
    "                              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "additive, n2v, consistency, w2v, hyper_all, hyper_quine = {}, {}, {}, {}, {}, {}\n",
    "\n",
    "model_dicts = [(\"n2v_additive\", additive), (\"n2v_standard\", n2v), (\"n2v_consistency\", consistency), \n",
    "               (\"w2v\", w2v), (\"hyper_all\", hyper_all), (\"hyper_quine\", hyper_quine)]\n",
    "for name, dict in model_dicts:\n",
    "    create_data_list(file_dict[name], dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_file = '..\\\\term_with_freq.csv'\n",
    "term_freq_dict = {}\n",
    "with open(target_file, 'r', encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for n, (_, term, freq) in enumerate(reader):\n",
    "        if n > 0:\n",
    "            term_freq_dict[term] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_all_refined = {}\n",
    "for target, values in hyper_all.items():\n",
    "    target_refined = target.replace(\"__xx\", \"\")\n",
    "    hyper_all_refined[target_refined] = values\n",
    "hyper_quine_refined = {}\n",
    "for target, values in hyper_quine.items():\n",
    "    target_refined = target.replace(\"__xx\", \"\")\n",
    "    hyper_quine_refined[target_refined] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_list = [additive, n2v, consistency, w2v]\n",
    "#target_list = ['physicalism', 'words', 'quantifiers', 'variables', 'maxim']\n",
    "model_dicts = [(\"n2v_additive\", additive), (\"n2v_standard\", n2v), (\"n2v_consistency\", consistency), \n",
    "               (\"w2v\", w2v), (\"hyper_all\", hyper_all_refined), (\"hyper_quine\", hyper_quine_refined)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Task 1 Stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stimuli(term, rank):\n",
    "    key = \"nn\" + str(rank)\n",
    "    result = {}\n",
    "    for name, model in model_dicts:\n",
    "        result[name] = model[term][key]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n2v_additive': \"('parts', 0.9998624324798584)\",\n",
       " 'n2v_standard': \"('parts', 0.9998587965965271)\",\n",
       " 'n2v_consistency': \"('relative_terms', 0.7685747146606445)\",\n",
       " 'w2v': \"('input', 0.6655906438827515)\",\n",
       " 'hyper_all': 'predicates(0.644643233777078)',\n",
       " 'hyper_quine': 'impressions(0.55739400834)'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example stimuli for target term 'ideas' on rank 2\n",
    "get_stimuli(\"ideas\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "term: objects    rank : 2      freq : 1985\n",
      "{'n2v_additive': \"('time', 0.9999998807907104)\", 'n2v_standard': \"('time', 0.9999998807907104)\", 'n2v_consistency': \"('observation_sentences', 0.7791314125061035)\", 'w2v': \"('generalization', 0.6695494055747986)\", 'hyper_all': 'numbers__xx(0.5614315542633023)', 'hyper_quine': 'physical(0.642044816049)'}\n",
      "-----------\n",
      "term: objects    rank : 5      freq : 1985\n",
      "{'n2v_additive': \"[('meaninglessness', 0.9999992847442627)]\", 'n2v_standard': \"[('intensional_abstraction', 0.9999990463256836)]\", 'n2v_consistency': \"[('variables', 0.5780197978019714)]\", 'w2v': \"('relented', 0.6466934680938721)\", 'hyper_all': 'predicates(0.5153655475019328)', 'hyper_quine': 'mobile(0.492253318375)'}\n",
      "-----------\n",
      "term: objects    rank : 50      freq : 1985\n",
      "{'n2v_additive': \"[('learning', 0.9949288964271545)]\", 'n2v_standard': \"[('specious_present', 0.993363618850708)]\", 'n2v_consistency': \"[('urelements', 0.48002809286117554)]\", 'w2v': \"('given', 0.5951076149940491)\", 'hyper_all': 'quantification(0.4463125879974395)', 'hyper_quine': 'blessed(0.278858678297)'}\n",
      "-----------\n",
      "term: quantification    rank : 2      freq : 1671\n",
      "{'n2v_additive': \"('construction', 0.9999727010726929)\", 'n2v_standard': \"('reality', 0.9999641180038452)\", 'n2v_consistency': \"('propositional_attitudes', 0.5921144485473633)\", 'w2v': \"('immanently', 0.6182039976119995)\", 'hyper_all': 'propositional(0.6740746426922295)', 'hyper_quine': 'existential(0.602559559971)'}\n",
      "-----------\n",
      "term: quantification    rank : 5      freq : 1671\n",
      "{'n2v_additive': \"[('translation', 0.9999597668647766)]\", 'n2v_standard': \"[('description', 0.9999463558197021)]\", 'n2v_consistency': \"[('intension', 0.44284480810165405)]\", 'w2v': \"('pattern', 0.6037701368331909)\", 'hyper_all': 'computable(0.6366847517204338)', 'hyper_quine': 'substitutional(0.48954648798)'}\n",
      "-----------\n",
      "term: quantification    rank : 50      freq : 1671\n",
      "{'n2v_additive': \"[('quantifiers', 0.9956674575805664)]\", 'n2v_standard': \"[('specious_present', 0.9940686225891113)]\", 'n2v_consistency': \"[('vocabularies', 0.24927040934562683)]\", 'w2v': \"('pstf', 0.5570308566093445)\", 'hyper_all': 'algebraically(0.5665595884438168)', 'hyper_quine': 'leftmost(0.290828412261)'}\n",
      "-----------\n",
      "term: about    rank : 2      freq : 1408\n",
      "{'n2v_additive': \"('propositional_attitudes', 0.9999983310699463)\", 'n2v_standard': \"('propositional_attitudes', 0.9999977946281433)\", 'n2v_consistency': \"('posit', 0.814532995223999)\", 'w2v': \"('designation', 0.694610595703125)\", 'hyper_all': 'intuitively(0.6522333546823541)', 'hyper_quine': 'regarding(0.334563634022)'}\n",
      "-----------\n",
      "term: about    rank : 5      freq : 1408\n",
      "{'n2v_additive': \"[('observation_sentences', 0.9999904036521912)]\", 'n2v_standard': \"[('observation_sentences', 0.9999877214431763)]\", 'n2v_consistency': \"[('adjectives', 0.582388699054718)]\", 'w2v': \"('reason', 0.6861124038696289)\", 'hyper_all': 'truthful(0.593712410124046)', 'hyper_quine': 'them(0.289637485701)'}\n",
      "-----------\n",
      "term: about    rank : 50      freq : 1408\n",
      "{'n2v_additive': \"[('learning', 0.9944681525230408)]\", 'n2v_standard': \"[('specious_present', 0.9928593635559082)]\", 'n2v_consistency': \"[('preconditions', 0.2114240527153015)]\", 'w2v': \"('quirks', 0.6340808868408203)\", 'hyper_all': 'sure(0.5220703617612229)', 'hyper_quine': 'information(0.209713430259)'}\n",
      "-----------\n",
      "term: meaning    rank : 2      freq : 1366\n",
      "{'n2v_additive': \"('transparency', 0.9999924302101135)\", 'n2v_standard': \"('transparency', 0.9999920129776001)\", 'n2v_consistency': \"('intensional_abstraction', 0.6337363123893738)\", 'w2v': \"('however', 0.6649000644683838)\", 'hyper_all': 'attributes__xx(0.6407686506470152)', 'hyper_quine': 'stimulus(0.67967604458)'}\n",
      "-----------\n",
      "term: meaning    rank : 5      freq : 1366\n",
      "{'n2v_additive': \"[('observation_sentences', 0.9999809265136719)]\", 'n2v_standard': \"[('observation_sentences', 0.9999762177467346)]\", 'n2v_consistency': \"[('words', 0.44318604469299316)]\", 'w2v': \"('disquote', 0.657448947429657)\", 'hyper_all': 'reality__xx(0.5773478153960551)', 'hyper_quine': 'ëgavagaií(0.560238123068)'}\n",
      "-----------\n",
      "term: meaning    rank : 50      freq : 1366\n",
      "{'n2v_additive': \"[('learning', 0.9943574070930481)]\", 'n2v_standard': \"[('specious_present', 0.9927600026130676)]\", 'n2v_consistency': \"[('gargling', 0.2582489550113678)]\", 'w2v': \"('committal', 0.5789175629615784)\", 'hyper_all': 'paradoxical(0.48592659868160865)', 'hyper_quine': 'unsullied(0.263428169814)'}\n",
      "-----------\n",
      "term: object    rank : 2      freq : 1287\n",
      "{'n2v_additive': \"('quantification', 0.9999688863754272)\", 'n2v_standard': \"('translation', 0.9999619722366333)\", 'n2v_consistency': \"('objects', 0.773756742477417)\", 'w2v': \"('incorrect', 0.6833862066268921)\", 'hyper_all': 'phoneme__xx(0.6530447399501204)', 'hyper_quine': 'objects(0.549032999713)'}\n",
      "-----------\n",
      "term: object    rank : 5      freq : 1287\n",
      "{'n2v_additive': \"[('adjectives', 0.9999526143074036)]\", 'n2v_standard': \"[('adjectives', 0.9999461770057678)]\", 'n2v_consistency': \"[('observation_sentences', 0.5572064518928528)]\", 'w2v': \"('irradiation', 0.6734964847564697)\", 'hyper_all': 'predicates(0.5295669353072836)', 'hyper_quine': 'physical(0.447278035145)'}\n",
      "-----------\n",
      "term: object    rank : 50      freq : 1287\n",
      "{'n2v_additive': \"[('learning', 0.9954464435577393)]\", 'n2v_standard': \"[('specious_present', 0.9939521551132202)]\", 'n2v_consistency': \"[('componentwise', 0.4047980308532715)]\", 'w2v': \"('wherefore', 0.6207870244979858)\", 'hyper_all': 'abstand(0.4597781470206242)', 'hyper_quine': 'unchangeable(0.252287359299)'}\n",
      "-----------\n",
      "term: time    rank : 2      freq : 1017\n",
      "{'n2v_additive': \"('attributes', 0.9999998807907104)\", 'n2v_standard': \"('objects', 0.9999998807907104)\", 'n2v_consistency': \"('sense_datum', 0.7301849722862244)\", 'w2v': \"('originally', 0.684440016746521)\", 'hyper_all': 'object__xx(0.5421348766292309)', 'hyper_quine': 'space(0.696565051777)'}\n",
      "-----------\n",
      "term: time    rank : 5      freq : 1017\n",
      "{'n2v_additive': \"[('meaninglessness', 0.9999993443489075)]\", 'n2v_standard': \"[('posit', 0.9999990463256836)]\", 'n2v_consistency': \"[('conditionals', 0.5768187642097473)]\", 'w2v': \"('avail', 0.6699494123458862)\", 'hyper_all': 'reality__xx(0.4726563724939055)', 'hyper_quine': 'diffuse(0.456960559593)'}\n",
      "-----------\n",
      "term: time    rank : 50      freq : 1017\n",
      "{'n2v_additive': \"[('learning', 0.9949171543121338)]\", 'n2v_standard': \"[('specious_present', 0.99335116147995)]\", 'n2v_consistency': '[(\"evola\\'s\", 0.33082786202430725)]', 'w2v': \"('are', 0.5768078565597534)\", 'hyper_all': 'intragenic(0.37298148672715314)', 'hyper_quine': 'consuming(0.266902239305)'}\n",
      "-----------\n",
      "term: translation    rank : 2      freq : 676\n",
      "{'n2v_additive': \"('intensional_abstraction', 0.9999953508377075)\", 'n2v_standard': \"('intensional_abstraction', 0.9999943971633911)\", 'n2v_consistency': \"('logical_particles', 0.7578331828117371)\", 'w2v': \"('words', 0.6150978803634644)\", 'hyper_all': 'learning__xx(0.6310236292105517)', 'hyper_quine': 'manual(0.565079882648)'}\n",
      "-----------\n",
      "term: translation    rank : 5      freq : 676\n",
      "{'n2v_additive': \"[('objects', 0.9999930262565613)]\", 'n2v_standard': \"[('objects', 0.999991774559021)]\", 'n2v_consistency': \"[('intensional_abstraction', 0.5981030464172363)]\", 'w2v': \"('god', 0.5999559760093689)\", 'hyper_all': 'negation(0.6107077146949895)', 'hyper_quine': 'indeterminacy(0.447629513195)'}\n",
      "-----------\n",
      "term: translation    rank : 50      freq : 676\n",
      "{'n2v_additive': \"[('learning', 0.9952461123466492)]\", 'n2v_standard': \"[('specious_present', 0.9937007427215576)]\", 'n2v_consistency': \"[('formula_224', 0.503150463104248)]\", 'w2v': \"('perspective', 0.5530391931533813)\", 'hyper_all': 'causation(0.5025980776457127)', 'hyper_quine': 'fairness(0.245990861546)'}\n",
      "-----------\n",
      "term: attributes    rank : 2      freq : 545\n",
      "{'n2v_additive': \"('time', 0.9999998807907104)\", 'n2v_standard': \"('time', 0.9999998211860657)\", 'n2v_consistency': \"('prediction', 0.6334723234176636)\", 'w2v': \"('staple', 0.6878774762153625)\", 'hyper_all': 'meaning__xx(0.6407686506470152)', 'hyper_quine': 'propositions(0.549754319724)'}\n",
      "-----------\n",
      "term: attributes    rank : 5      freq : 545\n",
      "{'n2v_additive': \"[('meaninglessness', 0.999999463558197)]\", 'n2v_standard': \"[('utterances', 0.9999991655349731)]\", 'n2v_consistency': \"[('numbers', 0.4369370639324188)]\", 'w2v': \"('spotting', 0.6216795444488525)\", 'hyper_all': 'logically(0.601047587511533)', 'hyper_quine': 'functions(0.394794231106)'}\n",
      "-----------\n",
      "term: attributes    rank : 50      freq : 545\n",
      "{'n2v_additive': \"[('learning', 0.9949374198913574)]\", 'n2v_standard': \"[('specious_present', 0.9933705925941467)]\", 'n2v_consistency': \"[('mmk', 0.01763315685093403)]\", 'w2v': \"('concentrating', 0.5506832599639893)\", 'hyper_all': 'mappings(0.5040686007733958)', 'hyper_quine': 'intension(0.242040958849)'}\n",
      "-----------\n",
      "term: ideas    rank : 2      freq : 444\n",
      "{'n2v_additive': \"('parts', 0.9998624324798584)\", 'n2v_standard': \"('parts', 0.9998587965965271)\", 'n2v_consistency': \"('relative_terms', 0.7685747146606445)\", 'w2v': \"('input', 0.6655906438827515)\", 'hyper_all': 'predicates(0.644643233777078)', 'hyper_quine': 'impressions(0.55739400834)'}\n",
      "-----------\n",
      "term: ideas    rank : 5      freq : 444\n",
      "{'n2v_additive': \"[('memory', 0.9997144937515259)]\", 'n2v_standard': \"[('memory', 0.9996693134307861)]\", 'n2v_consistency': \"[('adjectives', 0.5001513957977295)]\", 'w2v': \"('ruthlessly', 0.6281715631484985)\", 'hyper_all': 'attributes__xx(0.5691163861353427)', 'hyper_quine': 'interpolated(0.463617593575)'}\n",
      "-----------\n",
      "term: ideas    rank : 50      freq : 444\n",
      "{'n2v_additive': \"[('truth_vehicles', 0.9950840473175049)]\", 'n2v_standard': \"[('quantifiers', 0.9931572079658508)]\", 'n2v_consistency': \"[('rattler', -0.007654551416635513)]\", 'w2v': \"('attend', 0.5719764828681946)\", 'hyper_all': 'meaning__xx(0.4691307498557279)', 'hyper_quine': 'reflexes(0.276316946015)'}\n",
      "-----------\n",
      "term: paradox    rank : 2      freq : 438\n",
      "{'n2v_additive': \"('ordered_pair', 0.9995003938674927)\", 'n2v_standard': \"('ordered_pair', 0.9994755983352661)\", 'n2v_consistency': \"('subtraction', 0.636437177658081)\", 'w2v': \"('reflects', 0.6680153608322144)\", 'hyper_all': 'about__xx(0.5272853453997489)', 'hyper_quine': 'burali(0.628041485906)'}\n",
      "-----------\n",
      "term: paradox    rank : 5      freq : 438\n",
      "{'n2v_additive': \"[('meaning', 0.9994482398033142)]\", 'n2v_standard': \"[('meaning', 0.9994145631790161)]\", 'n2v_consistency': \"[('mentalistic', 0.47825074195861816)]\", 'w2v': \"('syncategorematic', 0.6392519474029541)\", 'hyper_all': 'prediction__xx(0.4589113346352214)', 'hyper_quine': 'packs(0.511914274996)'}\n",
      "-----------\n",
      "term: paradox    rank : 50      freq : 438\n",
      "{'n2v_additive': \"[('learning', 0.9934114217758179)]\", 'n2v_standard': \"[('specious_present', 0.9921902418136597)]\", 'n2v_consistency': \"[('relativism', 0.2657049000263214)]\", 'w2v': \"('only', 0.5765870809555054)\", 'hyper_all': 'why(0.37834959251949357)', 'hyper_quine': 'tenability(0.247683969622)'}\n",
      "-----------\n",
      "term: observation_sentences    rank : 2      freq : 422\n",
      "{'n2v_additive': \"('posit', 0.9999995231628418)\", 'n2v_standard': \"('posit', 0.9999993443489075)\", 'n2v_consistency': \"('variables', 0.7793430089950562)\", 'w2v': \"('despite', 0.6294688582420349)\", 'hyper_all': 'ideas__xx(0.5130738438025326)', 'hyper_quine': 'recantation(0.429783764399)'}\n",
      "-----------\n",
      "term: observation_sentences    rank : 5      freq : 422\n",
      "{'n2v_additive': \"[('meaninglessness', 0.999998152256012)]\", 'n2v_standard': \"[('meaninglessness', 0.9999975562095642)]\", 'n2v_consistency': \"[('ambiguity', 0.6702656745910645)]\", 'w2v': \"('word', 0.5981006622314453)\", 'hyper_all': 'orthogonality(0.45896982406846343)', 'hyper_quine': 'pegged(0.365165213403)'}\n",
      "-----------\n",
      "term: observation_sentences    rank : 50      freq : 422\n",
      "{'n2v_additive': \"[('learning', 0.9948699474334717)]\", 'n2v_standard': \"[('specious_present', 0.9932883381843567)]\", 'n2v_consistency': \"[('contextuality', 0.628420352935791)]\", 'w2v': \"('allegory', 0.5538179874420166)\", 'hyper_all': 'causality(0.38553279795420053)', 'hyper_quine': 'extensibility(0.23461418937)'}\n",
      "-----------\n",
      "term: construction    rank : 2      freq : 377\n",
      "{'n2v_additive': \"('nouns', 0.9999585747718811)\", 'n2v_standard': \"('nouns', 0.9999507069587708)\", 'n2v_consistency': \"('nouns', 0.5840516090393066)\", 'w2v': \"('variant', 0.6064723134040833)\", 'hyper_all': 'intuitively(0.6209325921267471)', 'hyper_quine': 'constructions(0.579642090198)'}\n",
      "-----------\n",
      "term: construction    rank : 5      freq : 377\n",
      "{'n2v_additive': \"[('conditionals', 0.999944806098938)]\", 'n2v_standard': \"[('conditionals', 0.999933123588562)]\", 'n2v_consistency': \"[('ideas', 0.42670974135398865)]\", 'w2v': \"('furry', 0.5936031341552734)\", 'hyper_all': 'quantifiers__xx(0.5934610995890073)', 'hyper_quine': 'hypotheticals(0.341417683463)'}\n",
      "-----------\n",
      "term: construction    rank : 50      freq : 377\n",
      "{'n2v_additive': \"[('maxim', 0.9956592917442322)]\", 'n2v_standard': \"[('specious_present', 0.9939136505126953)]\", 'n2v_consistency': \"[('formula_46', 0.1767955869436264)]\", 'w2v': \"('excise', 0.5491803288459778)\", 'hyper_all': 'ideas__xx(0.483079643461707)', 'hyper_quine': 'notation(0.192076969706)'}\n",
      "-----------\n",
      "term: application    rank : 2      freq : 357\n",
      "{'n2v_additive': \"('numbers', 0.999871015548706)\", 'n2v_standard': \"('subtraction', 0.9998617172241211)\", 'n2v_consistency': \"('ambiguity', 0.7322223782539368)\", 'w2v': \"('belong', 0.7035356163978577)\", 'hyper_all': 'multiplicity(0.568226521665545)', 'hyper_quine': 'abbreviative(0.472371679167)'}\n",
      "-----------\n",
      "term: application    rank : 5      freq : 357\n",
      "{'n2v_additive': \"[('variables', 0.9997222423553467)]\", 'n2v_standard': \"[('variables', 0.9996622800827026)]\", 'n2v_consistency': \"[('information', 0.5883387327194214)]\", 'w2v': \"('repeatable', 0.6894493103027344)\", 'hyper_all': 'martingale(0.5493002323398897)', 'hyper_quine': 'applications(0.410540836976)'}\n",
      "-----------\n",
      "term: application    rank : 50      freq : 357\n",
      "{'n2v_additive': \"[('divided_reference', 0.9915616512298584)]\", 'n2v_standard': \"[('learning', 0.9907624125480652)]\", 'n2v_consistency': \"[('find', 0.4157821536064148)]\", 'w2v': \"('ime', 0.6495540142059326)\", 'hyper_all': 'priori(0.4950984402442889)', 'hyper_quine': 'pm(0.213952546362)'}\n",
      "-----------\n",
      "term: description    rank : 2      freq : 350\n",
      "{'n2v_additive': \"('conditionals', 0.9999754428863525)\", 'n2v_standard': \"('conditionals', 0.9999715089797974)\", 'n2v_consistency': \"('observation_sentences', 0.5721513032913208)\", 'w2v': \"('borne', 0.6560813188552856)\", 'hyper_all': 'predicate(0.6341535203216879)', 'hyper_quine': 'descriptions(0.568429054472)'}\n",
      "-----------\n",
      "term: description    rank : 5      freq : 350\n",
      "{'n2v_additive': \"[('adjectives', 0.9999732375144958)]\", 'n2v_standard': \"[('adjectives', 0.9999690651893616)]\", 'n2v_consistency': \"[('variables', 0.4722336530685425)]\", 'w2v': \"('fxb', 0.6331598162651062)\", 'hyper_all': 'equivalence(0.5808161256910277)', 'hyper_quine': 'plural(0.414894513063)'}\n",
      "-----------\n",
      "term: description    rank : 50      freq : 350\n",
      "{'n2v_additive': \"[('learning', 0.9950604438781738)]\", 'n2v_standard': \"[('specious_present', 0.9934811592102051)]\", 'n2v_consistency': '[(\"noun\\'s\", 0.4001774191856384)]', 'w2v': \"('definitions', 0.5815335512161255)\", 'hyper_all': 'dft(0.5072837908111215)', 'hyper_quine': 'sententially(0.229760606654)'}\n",
      "-----------\n",
      "term: information    rank : 2      freq : 275\n",
      "{'n2v_additive': \"('learning', 0.9890459775924683)\", 'n2v_standard': \"('learning', 0.9877327680587769)\", 'n2v_consistency': \"('application', 0.5883387327194214)\", 'w2v': \"('reductions', 0.5541568994522095)\", 'hyper_all': 'ordered_pair__xx(0.5849625826866701)', 'hyper_quine': 'collateral(0.629120402529)'}\n",
      "-----------\n",
      "term: information    rank : 5      freq : 275\n",
      "{'n2v_additive': \"[('mentalistic', 0.987843930721283)]\", 'n2v_standard': \"[('mentalistic', 0.9865294694900513)]\", 'n2v_consistency': \"[('intensional_abstraction', 0.456917405128479)]\", 'w2v': \"('ôfõ', 0.5306439399719238)\", 'hyper_all': 'numbers__xx(0.5417978795611584)', 'hyper_quine': 'influences(0.433522125088)'}\n",
      "-----------\n",
      "term: information    rank : 50      freq : 275\n",
      "{'n2v_additive': \"[('numbers', 0.9829338788986206)]\", 'n2v_standard': \"[('application', 0.9811017513275146)]\", 'n2v_consistency': \"[('hear', 0.14770781993865967)]\", 'w2v': \"('studying', 0.4816551208496094)\", 'hyper_all': 'ambiguity__xx(0.4575310570163922)', 'hyper_quine': 'banal(0.22849456448)'}\n",
      "-----------\n",
      "term: parts    rank : 2      freq : 272\n",
      "{'n2v_additive': \"('sense_datum', 0.9999728202819824)\", 'n2v_standard': \"('sense_datum', 0.9999688863754272)\", 'n2v_consistency': \"('ambiguity', 0.8301409482955933)\", 'w2v': \"('total', 0.6527529358863831)\", 'hyper_all': 'objects__xx(0.5549399689469893)', 'hyper_quine': 'spatially(0.451261865581)'}\n",
      "-----------\n",
      "term: parts    rank : 5      freq : 272\n",
      "{'n2v_additive': \"[('object', 0.9998741149902344)]\", 'n2v_standard': \"[('ideas', 0.9998587965965271)]\", 'n2v_consistency': \"[('reality', 0.6579746603965759)]\", 'w2v': \"('invalidate', 0.6024715900421143)\", 'hyper_all': 'morphisms(0.4682825735362867)', 'hyper_quine': 'undetached(0.423431765295)'}\n",
      "-----------\n",
      "term: parts    rank : 50      freq : 272\n",
      "{'n2v_additive': \"[('specious_present', 0.9951412677764893)]\", 'n2v_standard': \"[('quantifiers', 0.9946000576019287)]\", 'n2v_consistency': \"[('conversationally', 0.46152371168136597)]\", 'w2v': \"('recognizable', 0.5133416652679443)\", 'hyper_all': 'lattices(0.39671620796102025)', 'hyper_quine': 'manifolds(0.224234643501)'}\n",
      "-----------\n",
      "term: truth_functions    rank : 2      freq : 267\n",
      "{'n2v_additive': \"('propositional_attitudes', 0.9999276399612427)\", 'n2v_standard': \"('propositional_attitudes', 0.9999222159385681)\", 'n2v_consistency': \"('variables', 0.7590671181678772)\", 'w2v': \"('immanently', 0.6422076225280762)\", 'hyper_all': 'conditionals__xx(0.5081501912849755)', 'hyper_quine': 'quantification(0.479450301849)'}\n",
      "-----------\n",
      "term: truth_functions    rank : 5      freq : 267\n",
      "{'n2v_additive': \"[('observation_sentences', 0.999910831451416)]\", 'n2v_standard': \"[('observation_sentences', 0.9999052286148071)]\", 'n2v_consistency': \"[('mental_states', 0.7206820845603943)]\", 'w2v': \"('have', 0.6224079728126526)\", 'hyper_all': 'orbicella(0.4587657758384603)', 'hyper_quine': 'exterior(0.357525084878)'}\n",
      "-----------\n",
      "term: truth_functions    rank : 50      freq : 267\n",
      "{'n2v_additive': \"[('learning', 0.9944412708282471)]\", 'n2v_standard': \"[('specious_present', 0.9929159283638)]\", 'n2v_consistency': '[(\"noun\\'s\", 0.5035247206687927)]', 'w2v': \"('hark', 0.5824824571609497)\", 'hyper_all': 'propositional(0.4033338920705628)', 'hyper_quine': 'irreparable(0.226736379764)'}\n",
      "-----------\n",
      "term: reduction    rank : 2      freq : 217\n",
      "{'n2v_additive': \"('pronouns', 0.9999784231185913)\", 'n2v_standard': \"('pronouns', 0.9999719858169556)\", 'n2v_consistency': \"('ordered_pair', 0.6487832069396973)\", 'w2v': \"('direct', 0.6604467034339905)\", 'hyper_all': 'quantification__xx(0.6007127541096816)', 'hyper_quine': 'inflations(0.3472099412)'}\n",
      "-----------\n",
      "term: reduction    rank : 5      freq : 217\n",
      "{'n2v_additive': \"[('about', 0.9999050498008728)]\", 'n2v_standard': \"[('about', 0.9999010562896729)]\", 'n2v_consistency': \"[('truth_vehicles', 0.42716848850250244)]\", 'w2v': \"('incommensurability', 0.6349347233772278)\", 'hyper_all': 'predicates(0.5614652951788447)', 'hyper_quine': 'epistemological(0.292736508813)'}\n",
      "-----------\n",
      "term: reduction    rank : 50      freq : 217\n",
      "{'n2v_additive': \"[('learning', 0.9933051466941833)]\", 'n2v_standard': \"[('truth_vehicles', 0.9923275113105774)]\", 'n2v_consistency': \"[('innuendos', 0.24720698595046997)]\", 'w2v': \"('prawitz', 0.6000109910964966)\", 'hyper_all': 'proofs(0.48983619813851426)', 'hyper_quine': 'immediate(0.213794169565)'}\n",
      "-----------\n",
      "term: reality    rank : 2      freq : 207\n",
      "{'n2v_additive': \"('nouns', 0.9999456405639648)\", 'n2v_standard': \"('nouns', 0.9999411702156067)\", 'n2v_consistency': \"('parts', 0.6579746603965759)\", 'w2v': \"('programming', 0.7062655687332153)\", 'hyper_all': 'monism(0.5890449600136162)', 'hyper_quine': 'monica(0.372022895373)'}\n",
      "-----------\n",
      "term: reality    rank : 5      freq : 207\n",
      "{'n2v_additive': \"[('translation', 0.9999123811721802)]\", 'n2v_standard': \"[('translation', 0.9999025464057922)]\", 'n2v_consistency': \"[('words', 0.4933135509490967)]\", 'w2v': \"('univocal', 0.6619195938110352)\", 'hyper_all': 'meaninglessness__xx(0.5255927040529246)', 'hyper_quine': 'stanford(0.254212569399)'}\n",
      "-----------\n",
      "term: reality    rank : 50      freq : 207\n",
      "{'n2v_additive': \"[('learning', 0.99535071849823)]\", 'n2v_standard': \"[('specious_present', 0.9936561584472656)]\", 'n2v_consistency': \"[('npcs', 0.23566192388534546)]\", 'w2v': \"('command', 0.5668536424636841)\", 'hyper_all': 'ontological(0.46456611998735386)', 'hyper_quine': 'kin(0.177529792852)'}\n",
      "-----------\n",
      "term: ordered_pair    rank : 2      freq : 191\n",
      "{'n2v_additive': \"('pronouns', 0.9999734163284302)\", 'n2v_standard': \"('pronouns', 0.9999668598175049)\", 'n2v_consistency': \"('truth_vehicles', 0.6313872337341309)\", 'w2v': \"('constitutes', 0.7229539752006531)\", 'hyper_all': 'information__xx(0.5849625826866701)', 'hyper_quine': 'insidious(0.449211142399)'}\n",
      "-----------\n",
      "term: ordered_pair    rank : 5      freq : 191\n",
      "{'n2v_additive': \"[('about', 0.9998800158500671)]\", 'n2v_standard': \"[('about', 0.9998810291290283)]\", 'n2v_consistency': \"[('observation_sentences', 0.41832435131073)]\", 'w2v': \"('loathe', 0.6558449268341064)\", 'hyper_all': 'numbers__xx(0.5271028749715306)', 'hyper_quine': 'ìoccurrenceî(0.320820077279)'}\n",
      "-----------\n",
      "term: ordered_pair    rank : 50      freq : 191\n",
      "{'n2v_additive': \"[('learning', 0.9930462837219238)]\", 'n2v_standard': \"[('truth_vehicles', 0.9924373626708984)]\", 'n2v_consistency': \"[('originalist', 0.3126331567764282)]\", 'w2v': \"('polar', 0.59228515625)\", 'hyper_all': 'homotopic(0.4379367072995394)', 'hyper_quine': 'systematisation(0.224214136828)'}\n",
      "-----------\n",
      "term: ambiguity    rank : 2      freq : 188\n",
      "{'n2v_additive': \"('nouns', 0.9995443820953369)\", 'n2v_standard': \"('nouns', 0.9994092583656311)\", 'n2v_consistency': \"('phoneme', 0.8540822267532349)\", 'w2v': \"('instantiation', 0.7148892283439636)\", 'hyper_all': 'predicate(0.5655492839440732)', 'hyper_quine': 'yey(0.586461304844)'}\n",
      "-----------\n",
      "term: ambiguity    rank : 5      freq : 188\n",
      "{'n2v_additive': \"[('adjectives', 0.9995035529136658)]\", 'n2v_standard': \"[('adjectives', 0.999364972114563)]\", 'n2v_consistency': \"[('application', 0.7322224378585815)]\", 'w2v': \"('earlier', 0.6648088693618774)\", 'hyper_all': 'suffices(0.5235851956692267)', 'hyper_quine': 'resolve(0.390473740723)'}\n",
      "-----------\n",
      "term: ambiguity    rank : 50      freq : 188\n",
      "{'n2v_additive': \"[('learning', 0.9946225881576538)]\", 'n2v_standard': \"[('specious_present', 0.9931604862213135)]\", 'n2v_consistency': \"[('__notoc__', 0.5589238405227661)]\", 'w2v': \"('tries', 0.610447883605957)\", 'hyper_all': 'infer(0.44334280318818003)', 'hyper_quine': 'detracts(0.209613308568)'}\n",
      "-----------\n",
      "term: conditionals    rank : 2      freq : 141\n",
      "{'n2v_additive': \"('adjectives', 0.9999998807907104)\", 'n2v_standard': \"('intensional_abstraction', 0.9999997615814209)\", 'n2v_consistency': \"('conditioning', 0.6936150789260864)\", 'w2v': \"('therewith', 0.60971999168396)\", 'hyper_all': 'predicates(0.6640054214985449)', 'hyper_quine': 'categoricals(0.536312252777)'}\n",
      "-----------\n",
      "term: conditionals    rank : 5      freq : 141\n",
      "{'n2v_additive': \"[('utterances', 0.9999994039535522)]\", 'n2v_standard': \"[('utterances', 0.9999991655349731)]\", 'n2v_consistency': \"[('truth_vehicles', 0.5053328275680542)]\", 'w2v': \"('dating', 0.6081440448760986)\", 'hyper_all': 'attributes__xx(0.5798823392480061)', 'hyper_quine': 'implied(0.417470789401)'}\n",
      "-----------\n",
      "term: conditionals    rank : 50      freq : 141\n",
      "{'n2v_additive': \"[('learning', 0.9950013160705566)]\", 'n2v_standard': \"[('specious_present', 0.9934340715408325)]\", 'n2v_consistency': '[(\"schulman\\'s\", 0.21150124073028564)]', 'w2v': \"('repeated', 0.5534606575965881)\", 'hyper_all': 'generalize(0.49131525828545175)', 'hyper_quine': 'applicational(0.235740071434)'}\n",
      "-----------\n",
      "term: propositional_attitudes    rank : 2      freq : 138\n",
      "{'n2v_additive': \"('transparency', 0.9999982118606567)\", 'n2v_standard': \"('about', 0.9999977350234985)\", 'n2v_consistency': \"('pronouns', 0.8141608834266663)\", 'w2v': \"('manifestation', 0.5634734630584717)\", 'hyper_all': 'attributes__xx(0.4629658582561398)', 'hyper_quine': 'irremediably(0.610459825018)'}\n",
      "-----------\n",
      "term: propositional_attitudes    rank : 5      freq : 138\n",
      "{'n2v_additive': \"[('meaning', 0.9999908208847046)]\", 'n2v_standard': \"[('logical_particles', 0.9999892711639404)]\", 'n2v_consistency': \"[('objects', 0.558279812335968)]\", 'w2v': \"('possible', 0.5516201257705688)\", 'hyper_all': 'application__xx(0.4172570580881146)', 'hyper_quine': 'thither(0.543511531061)'}\n",
      "-----------\n",
      "term: propositional_attitudes    rank : 50      freq : 138\n",
      "{'n2v_additive': \"[('learning', 0.9944831728935242)]\", 'n2v_standard': \"[('specious_present', 0.9928997755050659)]\", 'n2v_consistency': \"[('teleworking', 0.20410677790641785)]\", 'w2v': \"('hear', 0.4933311343193054)\", 'hyper_all': 'dialectical(0.3591847325691272)', 'hyper_quine': 'fused(0.266432618242)'}\n",
      "-----------\n",
      "term: nominalism    rank : 2      freq : 123\n",
      "{'n2v_additive': \"('parts', 0.9989736080169678)\", 'n2v_standard': \"('parts', 0.9989086985588074)\", 'n2v_consistency': \"('parts', 0.8482801914215088)\", 'w2v': \"('forth', 0.518254816532135)\", 'hyper_all': 'attributes__xx(0.5387967740160697)', 'hyper_quine': 'disavowals(0.294504559312)'}\n",
      "-----------\n",
      "term: nominalism    rank : 5      freq : 123\n",
      "{'n2v_additive': \"[('reality', 0.998828113079071)]\", 'n2v_standard': \"[('reality', 0.9987419247627258)]\", 'n2v_consistency': \"[('truth_functions', 0.7032061219215393)]\", 'w2v': \"('exist', 0.48702239990234375)\", 'hyper_all': 'posit__xx(0.4660723695075361)', 'hyper_quine': 'stine(0.286364877008)'}\n",
      "-----------\n",
      "term: nominalism    rank : 50      freq : 123\n",
      "{'n2v_additive': \"[('specious_present', 0.9940524101257324)]\", 'n2v_standard': \"[('quantifiers', 0.9925279021263123)]\", 'n2v_consistency': '[(\"you\\'d\", 0.49663621187210083)]', 'w2v': \"('tendentiously', 0.4389152526855469)\", 'hyper_all': 'individualism(0.397383498921411)', 'hyper_quine': 'claims(0.196160150225)'}\n",
      "-----------\n",
      "term: prediction    rank : 2      freq : 113\n",
      "{'n2v_additive': \"('parts', 0.9999655485153198)\", 'n2v_standard': \"('sense_datum', 0.9999618530273438)\", 'n2v_consistency': \"('numbers', 0.690130352973938)\", 'w2v': \"('names', 0.6315785646438599)\", 'hyper_all': 'posit__xx(0.6259397974903511)', 'hyper_quine': 'expectation(0.600630894724)'}\n",
      "-----------\n",
      "term: prediction    rank : 5      freq : 113\n",
      "{'n2v_additive': \"[('reality', 0.9999249577522278)]\", 'n2v_standard': \"[('reality', 0.999916672706604)]\", 'n2v_consistency': \"[('sense_datum', 0.5363653302192688)]\", 'w2v': \"('raven', 0.5978797674179077)\", 'hyper_all': 'about__xx(0.5540618675003941)', 'hyper_quine': 'confirms(0.465818583658)'}\n",
      "-----------\n",
      "term: prediction    rank : 50      freq : 113\n",
      "{'n2v_additive': \"[('quantifiers', 0.9950577020645142)]\", 'n2v_standard': \"[('specious_present', 0.994546115398407)]\", 'n2v_consistency': '[(\"riemann\\'s\", 0.2636508345603943)]', 'w2v': \"('discounting', 0.5539760589599609)\", 'hyper_all': 'cosmological(0.46592275824167484)', 'hyper_quine': 'sheer(0.222068471444)'}\n",
      "-----------\n",
      "term: dispositions    rank : 2      freq : 112\n",
      "{'n2v_additive': \"('divided_reference', 0.9988120198249817)\", 'n2v_standard': \"('divided_reference', 0.9987171292304993)\", 'n2v_consistency': \"('physicalism', 0.7778458595275879)\", 'w2v': \"('absolutely', 0.5811583995819092)\", 'hyper_all': 'translation__xx(0.6170622440909465)', 'hyper_quine': 'behaviour(0.641955548067)'}\n",
      "-----------\n",
      "term: dispositions    rank : 5      freq : 112\n",
      "{'n2v_additive': \"[('ideas', 0.9982466101646423)]\", 'n2v_standard': \"[('ideas', 0.9979704022407532)]\", 'n2v_consistency': \"[('learning', 0.6840236186981201)]\", 'w2v': \"('referred', 0.5607684850692749)\", 'hyper_all': 'information__xx(0.5474527036912896)', 'hyper_quine': 'manifestations(0.51971642537)'}\n",
      "-----------\n",
      "term: dispositions    rank : 50      freq : 112\n",
      "{'n2v_additive': \"[('truth_vehicles', 0.9904962778091431)]\", 'n2v_standard': \"[('information', 0.9872146248817444)]\", 'n2v_consistency': \"[('elizium23', 0.5700361728668213)]\", 'w2v': \"('case', 0.516817569732666)\", 'hyper_all': 'existential(0.44989147623991177)', 'hyper_quine': 'embodiments(0.240991129866)'}\n",
      "-----------\n",
      "term: pronouns    rank : 2      freq : 111\n",
      "{'n2v_additive': \"('subtraction', 0.999973714351654)\", 'n2v_standard': \"('ordered_pair', 0.9999668598175049)\", 'n2v_consistency': \"('about', 0.6425174474716187)\", 'w2v': \"('for', 0.6243113279342651)\", 'hyper_all': 'predicates(0.6100591301305387)', 'hyper_quine': 'exterior(0.544235182827)'}\n",
      "-----------\n",
      "term: pronouns    rank : 5      freq : 111\n",
      "{'n2v_additive': \"[('about', 0.9998717904090881)]\", 'n2v_standard': \"[('about', 0.9998492002487183)]\", 'n2v_consistency': \"[('truth_functions', 0.5317623615264893)]\", 'w2v': \"('learned', 0.5962779521942139)\", 'hyper_all': 'prepositions(0.49429717090395175)', 'hyper_quine': 'demonstratives(0.431730109007)'}\n",
      "-----------\n",
      "term: pronouns    rank : 50      freq : 111\n",
      "{'n2v_additive': \"[('divided_reference', 0.9929946660995483)]\", 'n2v_standard': \"[('learning', 0.9924057722091675)]\", 'n2v_consistency': \"[('sustainably', 0.29533639550209045)]\", 'w2v': \"('are', 0.5437353849411011)\", 'hyper_all': 'clarifying(0.4115588399525212)', 'hyper_quine': 'logical_particles(0.21436903792)'}\n",
      "-----------\n",
      "term: mentalistic    rank : 2      freq : 98\n",
      "{'n2v_additive': \"('ideas', 0.9997698068618774)\", 'n2v_standard': \"('ideas', 0.9997357130050659)\", 'n2v_consistency': \"('maxim', 0.6601607799530029)\", 'w2v': \"('contexts', 0.6314680576324463)\", 'hyper_all': 'utterance(0.5428638480902986)', 'hyper_quine': 'criticisms(0.402824963789)'}\n",
      "-----------\n",
      "term: mentalistic    rank : 5      freq : 98\n",
      "{'n2v_additive': \"[('prediction', 0.9996947050094604)]\", 'n2v_standard': \"[('prediction', 0.9996493458747864)]\", 'n2v_consistency': \"[('divided_reference', 0.5472625494003296)]\", 'w2v': \"('lexicon', 0.6142545938491821)\", 'hyper_all': 'truthful(0.5245556856405486)', 'hyper_quine': 'intensional(0.331862004484)'}\n",
      "-----------\n",
      "term: mentalistic    rank : 50      freq : 98\n",
      "{'n2v_additive': \"[('truth_vehicles', 0.9948755502700806)]\", 'n2v_standard': \"[('quantifiers', 0.9927363395690918)]\", 'n2v_consistency': \"[('caustically', 0.39720606803894043)]\", 'w2v': '(\"\\'\", 0.5563721656799316)', 'hyper_all': 'objectivity(0.4372729214898608)', 'hyper_quine': 'physicalism(0.21858057501)'}\n",
      "-----------\n",
      "term: conditioning    rank : 2      freq : 91\n",
      "{'n2v_additive': \"('sense_datum', 0.9999592900276184)\", 'n2v_standard': \"('sense_datum', 0.9999462366104126)\", 'n2v_consistency': \"('reduction', 0.4902670085430145)\", 'w2v': \"('does', 0.5304706692695618)\", 'hyper_all': 'cfbg(0.558460385147258)', 'hyper_quine': 'intake(0.505824625333)'}\n",
      "-----------\n",
      "term: conditioning    rank : 5      freq : 91\n",
      "{'n2v_additive': \"[('mental_states', 0.9998643398284912)]\", 'n2v_standard': \"[('mental_states', 0.9998255968093872)]\", 'n2v_consistency': \"[('attributes', 0.2861337959766388)]\", 'w2v': \"('swan', 0.510055422782898)\", 'hyper_all': 'evri(0.550397587744652)', 'hyper_quine': 'association(0.426773252484)'}\n",
      "-----------\n",
      "term: conditioning    rank : 50      freq : 91\n",
      "{'n2v_additive': \"[('specious_present', 0.9954869151115417)]\", 'n2v_standard': \"[('quantifiers', 0.9941815137863159)]\", 'n2v_consistency': \"[('information', 0.229167178273201)]\", 'w2v': \"('exceptional', 0.4386444687843323)\", 'hyper_all': 'najadi(0.48398298357845976)', 'hyper_quine': 'gerade(0.233624398963)'}\n",
      "-----------\n",
      "term: utterances    rank : 2      freq : 84\n",
      "{'n2v_additive': \"('adjectives', 0.999999463558197)\", 'n2v_standard': \"('objects', 0.9999994039535522)\", 'n2v_consistency': \"('nouns', 0.6774951219558716)\", 'w2v': \"('ostension', 0.7477714419364929)\", 'hyper_all': 'learning__xx(0.5495097682158279)', 'hyper_quine': 'sounded(0.396058705181)'}\n",
      "-----------\n",
      "term: utterances    rank : 5      freq : 84\n",
      "{'n2v_additive': \"[('logical_particles', 0.9999990463256836)]\", 'n2v_standard': \"[('logical_particles', 0.9999986886978149)]\", 'n2v_consistency': \"[('specious_present', 0.468923956155777)]\", 'w2v': \"('honorary', 0.6115363836288452)\", 'hyper_all': 'comprehend(0.4852501503905988)', 'hyper_quine': 'segments(0.297970704001)'}\n",
      "-----------\n",
      "term: utterances    rank : 50      freq : 84\n",
      "{'n2v_additive': \"[('learning', 0.9949877262115479)]\", 'n2v_standard': \"[('specious_present', 0.9934197664260864)]\", 'n2v_consistency': \"[('13s', 0.026671363040804863)]\", 'w2v': \"('individuating', 0.48001837730407715)\", 'hyper_all': 'gespeg(0.401397235612617)', 'hyper_quine': 'subjectís(0.196679352837)'}\n",
      "-----------\n",
      "term: adjectives    rank : 2      freq : 71\n",
      "{'n2v_additive': \"('attributes', 0.9999996423721313)\", 'n2v_standard': \"('intensional_abstraction', 0.9999996423721313)\", 'n2v_consistency': \"('nouns', 0.7453526854515076)\", 'w2v': \"('praised', 0.7090355157852173)\", 'hyper_all': 'nouns__xx(0.5959348802593132)', 'hyper_quine': 'attributive(0.561285932498)'}\n",
      "-----------\n",
      "term: adjectives    rank : 5      freq : 71\n",
      "{'n2v_additive': \"[('utterances', 0.999999463558197)]\", 'n2v_standard': \"[('utterances', 0.9999994039535522)]\", 'n2v_consistency': \"[('posit', 0.5111610293388367)]\", 'w2v': \"('catenands', 0.6564942002296448)\", 'hyper_all': 'inflect(0.5572484501343201)', 'hyper_quine': 'poor(0.416259894996)'}\n",
      "-----------\n",
      "term: adjectives    rank : 50      freq : 71\n",
      "{'n2v_additive': \"[('learning', 0.994995653629303)]\", 'n2v_standard': \"[('specious_present', 0.993425726890564)]\", 'n2v_consistency': \"[('fractional', 0.09157919138669968)]\", 'w2v': \"('exclusive', 0.6101387739181519)\", 'hyper_all': 'pronouns__xx(0.4515984322141335)', 'hyper_quine': 'tantamount(0.199280613271)'}\n",
      "-----------\n",
      "term: memory    rank : 2      freq : 55\n",
      "{'n2v_additive': \"('mentalistic', 0.9995752573013306)\", 'n2v_standard': \"('mentalistic', 0.9995152950286865)\", 'n2v_consistency': \"('reduction', 0.5213330984115601)\", 'w2v': \"('vira', 0.5654861330986023)\", 'hyper_all': 'whoit(0.5091151868470429)', 'hyper_quine': 'contiguity(0.303563945534)'}\n",
      "-----------\n",
      "term: memory    rank : 5      freq : 55\n",
      "{'n2v_additive': \"[('prediction', 0.9993042945861816)]\", 'n2v_standard': \"[('prediction', 0.9992390871047974)]\", 'n2v_consistency': \"[('conditioning', 0.4233762323856354)]\", 'w2v': \"('crude', 0.5126442909240723)\", 'hyper_all': 'muranda(0.4560760674918819)', 'hyper_quine': 'conceptualizations(0.250230107273)'}\n",
      "-----------\n",
      "term: memory    rank : 50      freq : 55\n",
      "{'n2v_additive': \"[('truth_vehicles', 0.9942007660865784)]\", 'n2v_standard': \"[('quantifiers', 0.991370677947998)]\", 'n2v_consistency': \"[('heartsick', 0.3478661775588989)]\", 'w2v': \"('operate', 0.4502229690551758)\", 'hyper_all': 'theyll(0.40895948323463316)', 'hyper_quine': 'trace(0.180725137427)'}\n",
      "-----------\n",
      "term: confirmation    rank : 2      freq : 49\n",
      "{'n2v_additive': \"('about', 0.9999709129333496)\", 'n2v_standard': \"('about', 0.9999639987945557)\", 'n2v_consistency': \"('specious_present', 0.5991045236587524)\", 'w2v': \"('slater', 0.6395320892333984)\", 'hyper_all': 'plausibility(0.6514146002313748)', 'hyper_quine': 'disconfirmation(0.371657017454)'}\n",
      "-----------\n",
      "term: confirmation    rank : 5      freq : 49\n",
      "{'n2v_additive': \"[('reduction', 0.9999485611915588)]\", 'n2v_standard': \"[('intension', 0.9999403953552246)]\", 'n2v_consistency': \"[('meaninglessness', 0.3460054397583008)]\", 'w2v': \"('ëgxí', 0.5962320566177368)\", 'hyper_all': 'construction__xx(0.5388460577804624)', 'hyper_quine': 'elaborateness(0.294806608872)'}\n",
      "-----------\n",
      "term: confirmation    rank : 50      freq : 49\n",
      "{'n2v_additive': \"[('learning', 0.9941227436065674)]\", 'n2v_standard': \"[('specious_present', 0.9924834966659546)]\", 'n2v_consistency': \"[('144', 0.03859987482428551)]\", 'w2v': \"('archaeological', 0.5292482376098633)\", 'hyper_all': 'intuition(0.46324068462445034)', 'hyper_quine': 'edges(0.207944178888)'}\n",
      "-----------\n",
      "term: posit    rank : 2      freq : 47\n",
      "{'n2v_additive': \"('observation_sentences', 0.9999995231628418)\", 'n2v_standard': \"('observation_sentences', 0.9999993443489075)\", 'n2v_consistency': \"('about', 0.814532995223999)\", 'w2v': \"('impressions', 0.5929021835327148)\", 'hyper_all': 'prediction__xx(0.6259397974903511)', 'hyper_quine': 'positing(0.377990235215)'}\n",
      "-----------\n",
      "term: posit    rank : 5      freq : 47\n",
      "{'n2v_additive': \"[('meaninglessness', 0.9999986886978149)]\", 'n2v_standard': \"[('meaninglessness', 0.9999982714653015)]\", 'n2v_consistency': \"[('sense_datum', 0.5190473794937134)]\", 'w2v': \"('redundancies', 0.575054407119751)\", 'hyper_all': 'anyones(0.5917023964437924)', 'hyper_quine': 'preeminently(0.336527280619)'}\n",
      "-----------\n",
      "term: posit    rank : 50      freq : 47\n",
      "{'n2v_additive': \"[('learning', 0.9948250651359558)]\", 'n2v_standard': \"[('specious_present', 0.9932495355606079)]\", 'n2v_consistency': \"[('formula_186', 0.4332887530326843)]\", 'w2v': \"('connote', 0.5295965671539307)\", 'hyper_all': 'ignore(0.5098733509633344)', 'hyper_quine': 'denoted(0.217393183527)'}\n",
      "-----------\n",
      "term: subtraction    rank : 2      freq : 42\n",
      "{'n2v_additive': \"('reduction', 0.9999730587005615)\", 'n2v_standard': \"('reduction', 0.9999603033065796)\", 'n2v_consistency': \"('truth_vehicles', 0.8543700575828552)\", 'w2v': \"('avoided', 0.6143591403961182)\", 'hyper_all': 'ordinal(0.6040513011118614)', 'hyper_quine': 'addition(0.49202148092)'}\n",
      "-----------\n",
      "term: subtraction    rank : 5      freq : 42\n",
      "{'n2v_additive': \"[('meaning', 0.9998419284820557)]\", 'n2v_standard': \"[('numbers', 0.9998225569725037)]\", 'n2v_consistency': \"[('sense_datum', 0.5349727869033813)]\", 'w2v': \"('fields', 0.5687719583511353)\", 'hyper_all': 'equivalence(0.5661195475902847)', 'hyper_quine': 'negation(0.413245516778)'}\n",
      "-----------\n",
      "term: subtraction    rank : 50      freq : 42\n",
      "{'n2v_additive': \"[('learning', 0.9926812052726746)]\", 'n2v_standard': \"[('learning', 0.992010235786438)]\", 'n2v_consistency': \"[('agnosticism', 0.43604815006256104)]\", 'w2v': \"('justify', 0.5158969759941101)\", 'hyper_all': 'quadratic(0.5271292252336661)', 'hyper_quine': 'tonight(0.21385012862)'}\n",
      "-----------\n",
      "term: meaninglessness    rank : 2      freq : 40\n",
      "{'n2v_additive': \"('objects', 0.9999994039535522)\", 'n2v_standard': \"('time', 0.999998927116394)\", 'n2v_consistency': \"('information', 0.47685086727142334)\", 'w2v': \"('quite', 0.5579918622970581)\", 'hyper_all': 'vanatr(0.6042639411751676)', 'hyper_quine': 'notion(0.324156450594)'}\n",
      "-----------\n",
      "term: meaninglessness    rank : 5      freq : 40\n",
      "{'n2v_additive': \"[('intensional_abstraction', 0.9999990463256836)]\", 'n2v_standard': \"[('intensional_abstraction', 0.9999983310699463)]\", 'n2v_consistency': \"[('maxim', 0.3164089620113373)]\", 'w2v': \"('defectiveness', 0.5043354630470276)\", 'hyper_all': 'pillock(0.5760786889443317)', 'hyper_quine': 'grading(0.289281694017)'}\n",
      "-----------\n",
      "term: meaninglessness    rank : 50      freq : 40\n",
      "{'n2v_additive': \"[('learning', 0.9949236512184143)]\", 'n2v_standard': \"[('specious_present', 0.9933179616928101)]\", 'n2v_consistency': \"[('truth_functions', 0.17939269542694092)]\", 'w2v': \"('suspicion', 0.4569425880908966)\", 'hyper_all': 'coolitude(0.47424935317482475)', 'hyper_quine': 'grief(0.212138485382)'}\n",
      "-----------\n",
      "term: nouns    rank : 2      freq : 38\n",
      "{'n2v_additive': \"('conditionals', 0.9999858140945435)\", 'n2v_standard': \"('translation', 0.9999822974205017)\", 'n2v_consistency': \"('utterances', 0.6774950623512268)\", 'w2v': \"('starting', 0.6334487199783325)\", 'hyper_all': 'adjectives__xx(0.5959348802593132)', 'hyper_quine': 'adjectives(0.41265740931)'}\n",
      "-----------\n",
      "term: nouns    rank : 5      freq : 38\n",
      "{'n2v_additive': \"[('meaninglessness', 0.9999827146530151)]\", 'n2v_standard': \"[('object', 0.9999776482582092)]\", 'n2v_consistency': \"[('words', 0.47131961584091187)]\", 'w2v': \"('fied', 0.6018933653831482)\", 'hyper_all': 'periphrastic(0.5721691014442677)', 'hyper_quine': 'prepositions(0.33419333636)'}\n",
      "-----------\n",
      "term: nouns    rank : 50      freq : 38\n",
      "{'n2v_additive': \"[('learning', 0.9951952695846558)]\", 'n2v_standard': \"[('specious_present', 0.9936231374740601)]\", 'n2v_consistency': \"[('occupancies', 0.10680995881557465)]\", 'w2v': \"('window', 0.5546831488609314)\", 'hyper_all': 'precede(0.4760107804636865)', 'hyper_quine': 'widens(0.194504455819)'}\n",
      "-----------\n",
      "term: intension    rank : 2      freq : 37\n",
      "{'n2v_additive': \"('logical_particles', 0.9999971389770508)\", 'n2v_standard': \"('logical_particles', 0.9999963045120239)\", 'n2v_consistency': \"('words', 0.6800811290740967)\", 'w2v': \"('complements', 0.6599233150482178)\", 'hyper_all': 'haneullim(0.607244012647917)', 'hyper_quine': 'inconsistencies(0.316838300291)'}\n",
      "-----------\n",
      "term: intension    rank : 5      freq : 37\n",
      "{'n2v_additive': \"[('time', 0.9999954700469971)]\", 'n2v_standard': \"[('time', 0.9999940395355225)]\", 'n2v_consistency': \"[('variables', 0.5284697413444519)]\", 'w2v': \"('thing', 0.6012294292449951)\", 'hyper_all': 'preslavets(0.5782869385679285)', 'hyper_quine': 'acquiescence(0.28962575829)'}\n",
      "-----------\n",
      "term: intension    rank : 50      freq : 37\n",
      "{'n2v_additive': \"[('learning', 0.9946820139884949)]\", 'n2v_standard': \"[('specious_present', 0.9931345582008362)]\", 'n2v_consistency': \"[('morphophonemic', 0.4480642080307007)]\", 'w2v': \"('linger', 0.5416672825813293)\", 'hyper_all': 'lutum(0.5177091036547257)', 'hyper_quine': 'paternal(0.213134212302)'}\n",
      "-----------\n",
      "term: logical_particles    rank : 2      freq : 33\n",
      "{'n2v_additive': \"('observation_sentences', 0.9999995231628418)\", 'n2v_standard': \"('observation_sentences', 0.9999994039535522)\", 'n2v_consistency': \"('variables', 0.7228307723999023)\", 'w2v': \"('universe', 0.737293004989624)\", 'hyper_all': 'categorematic(0.5464946167933944)', 'hyper_quine': 'clauses(0.318213451737)'}\n",
      "-----------\n",
      "term: logical_particles    rank : 5      freq : 33\n",
      "{'n2v_additive': \"[('utterances', 0.999998927116394)]\", 'n2v_standard': \"[('attributes', 0.9999984502792358)]\", 'n2v_consistency': '[(\"hadamard\\'s\", 0.5957897901535034)]', 'w2v': \"('method', 0.6347755789756775)\", 'hyper_all': 'methylprednisolone(0.5075854439224136)', 'hyper_quine': 'numberhood(0.273288355532)'}\n",
      "-----------\n",
      "term: logical_particles    rank : 50      freq : 33\n",
      "{'n2v_additive': \"[('learning', 0.9948855638504028)]\", 'n2v_standard': \"[('specious_present', 0.9933205246925354)]\", 'n2v_consistency': '[(\"fubini\\'s\", 0.5665344595909119)]', 'w2v': \"('conventional', 0.5474004745483398)\", 'hyper_all': 'ponens(0.41253336267857965)', 'hyper_quine': 'ôfx(0.200691638703)'}\n",
      "-----------\n",
      "term: relative_terms    rank : 2      freq : 29\n",
      "{'n2v_additive': \"('divided_reference', 0.9974598288536072)\", 'n2v_standard': \"('divided_reference', 0.9970896244049072)\", 'n2v_consistency': \"('ideas', 0.7685747146606445)\", 'w2v': \"('protosyntactical', 0.689300000667572)\", 'hyper_all': 'conditionals__xx(0.46207849047324345)', 'hyper_quine': 'spaced(0.477224736497)'}\n",
      "-----------\n",
      "term: relative_terms    rank : 5      freq : 29\n",
      "{'n2v_additive': \"[('conditioning', 0.997035026550293)]\", 'n2v_standard': \"[('mentalistic', 0.9967713952064514)]\", 'n2v_consistency': \"[('confirmation', 0.3916059732437134)]\", 'w2v': \"('eponymous', 0.6743100881576538)\", 'hyper_all': 'tetrafluoroethylene(0.44761299783672)', 'hyper_quine': 'singing(0.383262806695)'}\n",
      "-----------\n",
      "term: relative_terms    rank : 50      freq : 29\n",
      "{'n2v_additive': \"[('truth_vehicles', 0.9921378493309021)]\", 'n2v_standard': \"[('quantifiers', 0.9880157709121704)]\", 'n2v_consistency': \"[('conn', -0.08764718472957611)]\", 'w2v': \"('perhaps', 0.6179524064064026)\", 'hyper_all': 'unipotent(0.3840597990140112)', 'hyper_quine': 'informally(0.258234550176)'}\n",
      "-----------\n",
      "term: mental_states    rank : 2      freq : 23\n",
      "{'n2v_additive': \"('translation', 0.9999146461486816)\", 'n2v_standard': \"('translation', 0.9999041557312012)\", 'n2v_consistency': \"('nominalism', 0.7509139776229858)\", 'w2v': \"('breadth', 0.5983171463012695)\", 'hyper_all': 'subsemigroups(0.6576518534377681)', 'hyper_quine': 'states(0.553216700339)'}\n",
      "-----------\n",
      "term: mental_states    rank : 5      freq : 23\n",
      "{'n2v_additive': \"[('sense_datum', 0.9998910427093506)]\", 'n2v_standard': \"[('sense_datum', 0.9998736381530762)]\", 'n2v_consistency': \"[('parts', 0.6944733262062073)]\", 'w2v': \"('maximizing', 0.5816681981086731)\", 'hyper_all': 'quinian(0.5637796279452003)', 'hyper_quine': 'happenings(0.430728079526)'}\n",
      "-----------\n",
      "term: mental_states    rank : 50      freq : 23\n",
      "{'n2v_additive': \"[('quantifiers', 0.9949883222579956)]\", 'n2v_standard': \"[('specious_present', 0.9944741129875183)]\", 'n2v_consistency': \"[('diffmode', 0.522758424282074)]\", 'w2v': \"('weaker', 0.5366246104240417)\", 'hyper_all': 'moonwalks(0.4962238226881959)', 'hyper_quine': 'stretches(0.218338509141)'}\n",
      "-----------\n",
      "term: transparency    rank : 2      freq : 18\n",
      "{'n2v_additive': \"('propositional_attitudes', 0.9999980330467224)\", 'n2v_standard': \"('propositional_attitudes', 0.9999979734420776)\", 'n2v_consistency': \"('intensional_abstraction', 0.4400668144226074)\", 'w2v': \"('gtm', 0.5782111883163452)\", 'hyper_all': 'pitches..(0.5554788012749582)', 'hyper_quine': 'unproblematical(0.350420201412)'}\n",
      "-----------\n",
      "term: transparency    rank : 5      freq : 18\n",
      "{'n2v_additive': \"[('logical_particles', 0.9999937415122986)]\", 'n2v_standard': \"[('posit', 0.9999920129776001)]\", 'n2v_consistency': \"[('decreased', 0.2555401027202606)]\", 'w2v': \"('dt', 0.5360708236694336)\", 'hyper_all': 'helenalin(0.4918789688202927)', 'hyper_quine': 'alps(0.318995680977)'}\n",
      "-----------\n",
      "term: transparency    rank : 50      freq : 18\n",
      "{'n2v_additive': \"[('learning', 0.9946048259735107)]\", 'n2v_standard': \"[('specious_present', 0.9929605722427368)]\", 'n2v_consistency': \"[('μl', 0.20426607131958008)]\", 'w2v': \"('localized', 0.473137229681015)\", 'hyper_all': 'countershaded(0.42414404005882866)', 'hyper_quine': 'wary(0.242166631335)'}\n",
      "-----------\n",
      "term: truth_vehicles    rank : 2      freq : 14\n",
      "{'n2v_additive': \"('pronouns', 0.9967563152313232)\", 'n2v_standard': \"('numbers', 0.9929383993148804)\", 'n2v_consistency': \"('numbers', 0.7420178651809692)\", 'w2v': \"('pernicious', 0.5076371431350708)\", 'hyper_all': 'vpcs(0.6043907498471554)', 'hyper_quine': 'surrogate(0.490350721199)'}\n",
      "-----------\n",
      "term: truth_vehicles    rank : 5      freq : 14\n",
      "{'n2v_additive': \"[('description', 0.9966405630111694)]\", 'n2v_standard': \"[('truth_functions', 0.9924196004867554)]\", 'n2v_consistency': \"[('paradox', 0.5470851063728333)]\", 'w2v': \"('eskimos', 0.4796900749206543)\", 'hyper_all': 'hachuring(0.5125858070644356)', 'hyper_quine': 'deflationist(0.314001814108)'}\n",
      "-----------\n",
      "term: truth_vehicles    rank : 50      freq : 14\n",
      "{'n2v_additive': \"[('learning', 0.9892294406890869)]\", 'n2v_standard': \"[('specious_present', 0.9830658435821533)]\", 'n2v_consistency': \"[('objectivism', 0.37760472297668457)]\", 'w2v': \"('enact', 0.408457487821579)\", 'hyper_all': 'niezabitowska(0.44984899406340334)', 'hyper_quine': 'universals(0.223798885885)'}\n",
      "-----------\n",
      "term: intensional_abstraction    rank : 2      freq : 9\n",
      "{'n2v_additive': \"('adjectives', 0.9999996423721313)\", 'n2v_standard': \"('adjectives', 0.9999996423721313)\", 'n2v_consistency': \"('meaning', 0.6337363123893738)\", 'w2v': \"('wre', 0.6475462913513184)\", 'hyper_all': 'sportband(0.6219895482264055)', 'hyper_quine': 'referen(0.366349076361)'}\n",
      "-----------\n",
      "term: intensional_abstraction    rank : 5      freq : 9\n",
      "{'n2v_additive': \"[('meaninglessness', 0.9999990463256836)]\", 'n2v_standard': \"[('utterances', 0.9999985694885254)]\", 'n2v_consistency': \"[('nominalism', 0.5052311420440674)]\", 'w2v': \"('referents', 0.630821943283081)\", 'hyper_all': 'equipotentiality(0.5687634988671549)', 'hyper_quine': 'opaque(0.330764930432)'}\n",
      "-----------\n",
      "term: intensional_abstraction    rank : 50      freq : 9\n",
      "{'n2v_additive': \"[('learning', 0.9950168132781982)]\", 'n2v_standard': \"[('specious_present', 0.9934431314468384)]\", 'n2v_consistency': '[(\"yak\\'s\", 0.33578935265541077)]', 'w2v': \"('theorize', 0.5774198174476624)\", 'hyper_all': 'wannier(0.47764477686242357)', 'hyper_quine': 'quantified(0.227276052442)'}\n",
      "-----------\n",
      "term: sense_datum    rank : 2      freq : 7\n",
      "{'n2v_additive': \"('prediction', 0.9999662637710571)\", 'n2v_standard': \"('prediction', 0.9999618530273438)\", 'n2v_consistency': \"('construction', 0.5611881017684937)\", 'w2v': \"('picks', 0.5939058065414429)\", 'hyper_all': 'knaanic(0.6208134864160203)', 'hyper_quine': 'flimsy(0.402284928507)'}\n",
      "-----------\n",
      "term: sense_datum    rank : 5      freq : 7\n",
      "{'n2v_additive': \"[('mental_states', 0.9998910427093506)]\", 'n2v_standard': \"[('mental_states', 0.999873697757721)]\", 'n2v_consistency': \"[('posit', 0.5190473794937134)]\", 'w2v': \"('making', 0.5570008754730225)\", 'hyper_all': 'kurmali(0.5927666209615696)', 'hyper_quine': 'ironical(0.34203732523)'}\n",
      "-----------\n",
      "term: sense_datum    rank : 50      freq : 7\n",
      "{'n2v_additive': \"[('specious_present', 0.9952532052993774)]\", 'n2v_standard': \"[('quantifiers', 0.9945510625839233)]\", 'n2v_consistency': \"[('euler', 0.24344290792942047)]\", 'w2v': \"('frees', 0.4713868498802185)\", 'hyper_all': 'mavea(0.5596062691728994)', 'hyper_quine': 'johns(0.242834194967)'}\n"
     ]
    }
   ],
   "source": [
    "#get stimuli for all terms on all ranks, order of stimuli was randomized in the experiment\n",
    "#only stimuli in the relatedness terms list were used\n",
    "for term, freq in term_freq_dict.items():\n",
    "    for rank in [2,5,50]:\n",
    "        print(\"-----------\")\n",
    "        print(\"term: \" + term + \"    rank : \" + str(rank) + \"      freq : \" + str(freq))\n",
    "        print(get_stimuli(term, rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 objects 1985\n",
      "2 quantification 1671\n",
      "3 about 1408\n",
      "4 meaning 1366\n",
      "5 object 1287\n",
      "6 time 1017\n",
      "7 translation 676\n",
      "8 attributes 545\n",
      "9 ideas 444\n",
      "10 paradox 438\n",
      "11 observation_sentences 422\n",
      "12 construction 377\n",
      "13 application 357\n",
      "14 description 350\n",
      "15 information 275\n",
      "16 parts 272\n",
      "17 truth_functions 267\n",
      "18 reduction 217\n",
      "19 reality 207\n",
      "20 ordered_pair 191\n",
      "21 ambiguity 188\n",
      "22 conditionals 141\n",
      "23 propositional_attitudes 138\n",
      "24 nominalism 123\n",
      "25 prediction 113\n",
      "26 dispositions 112\n",
      "27 pronouns 111\n",
      "28 mentalistic 98\n",
      "29 conditioning 91\n",
      "30 utterances 84\n",
      "31 adjectives 71\n",
      "32 memory 55\n",
      "33 confirmation 49\n",
      "34 posit 47\n",
      "35 subtraction 42\n",
      "36 meaninglessness 40\n",
      "37 nouns 38\n",
      "38 intension 37\n",
      "39 logical_particles 33\n",
      "40 relative_terms 29\n",
      "41 mental_states 23\n",
      "42 transparency 18\n",
      "43 truth_vehicles 14\n",
      "44 intensional_abstraction 9\n",
      "45 sense_datum 7\n"
     ]
    }
   ],
   "source": [
    "#all terget words\n",
    "counter = 1\n",
    "wordlist = []\n",
    "for term, freq in term_freq_dict.items():\n",
    "        print(counter, term, freq )\n",
    "        counter += 1\n",
    "        wordlist.append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terms synonym detection task:\n",
      "\n",
      "high frequency:\n",
      "1, objects, 1985\n",
      "2, quantification, 1671\n",
      "3, about, 1408\n",
      "4, meaning, 1366\n",
      "5, object, 1287\n",
      "6, time, 1017\n",
      "7, translation, 676\n",
      "8, attributes, 545\n",
      "9, ideas, 444\n",
      "10, paradox, 438\n",
      "11, observation_sentences, 422\n",
      "12, construction, 377\n",
      "13, application, 357\n",
      "14, description, 350\n",
      "15, information, 275\n",
      "\n",
      "low frequency:\n",
      "16, utterances, 84\n",
      "17, adjectives, 71\n",
      "18, memory, 55\n",
      "19, confirmation, 49\n",
      "20, posit, 47\n",
      "21, subtraction, 42\n",
      "22, meaninglessness, 40\n",
      "23, nouns, 38\n",
      "24, intension, 37\n",
      "25, logical_particles, 33\n",
      "26, relative_terms, 29\n",
      "27, mental_states, 23\n",
      "28, transparency, 18\n",
      "29, truth_vehicles, 14\n",
      "30, intensional_abstraction, 9\n",
      "31, sense_datum, 7\n"
     ]
    }
   ],
   "source": [
    "relatedness_terms = ['objects','quantification','about', 'meaning','object', 'time', 'translation', 'attributes', 'ideas', \n",
    "                     'paradox','observation_sentences', 'construction', 'application', 'description', 'information',\n",
    "                     'utterances', 'adjectives', 'memory', 'confirmation','posit','subtraction','meaninglessness','nouns',\n",
    "                    'intension','logical_particles','relative_terms','mental_states','transparency','truth_vehicles',\n",
    "                     'intensional_abstraction','sense_datum']\n",
    "\n",
    "new_words = ['construction', 'application', 'description', 'information',\n",
    "                     'utterances', 'adjectives', 'memory', 'confirmation','posit',]\n",
    "#terms used in relatedness task\n",
    "print(\"terms synonym detection task:\")\n",
    "print()\n",
    "print(\"high frequency:\")\n",
    "counter = 1\n",
    "for word in relatedness_terms:\n",
    "    if counter == 16:\n",
    "        print()\n",
    "        print(\"low frequency:\")\n",
    "    print(str(counter)+\",\", word+\",\", term_freq_dict[word])\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Outlier task stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outlier_stimuli(model, term):\n",
    "    print(\"nn1: \" + model[term][\"nn2\"])\n",
    "    print(\"nn2: \" + model[term][\"nn3\"])\n",
    "    print(\"outlier: \" + model[term][\"outlier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terms outlier task\n",
      "low freq:\n",
      "0, sense_datum, 7\n",
      "1, intensional_abstraction, 9\n",
      "2, truth_vehicles, 14\n",
      "3, conditioning, 91\n",
      "4, mentalistic, 98\n",
      "5, pronouns, 111\n",
      "6, dispositions, 112\n",
      "7, prediction, 113\n",
      "8, nominalism, 123\n",
      "9, propositional_attitudes, 138\n",
      "10, conditionals, 141\n",
      "\n",
      "high freq:\n",
      "11, ambiguity, 188\n",
      "12, ordered_pair, 191\n",
      "13, reality, 207\n",
      "14, reduction, 217\n",
      "15, truth_functions, 267\n",
      "16, parts, 272\n",
      "17, about, 1408\n",
      "18, quantification, 1671\n",
      "19, objects, 1985\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "outlier_terms = 0\n",
    "#for the outlier detection task we use all words not in synonym detection task + 3 high- and 3 low frequency words\n",
    "outlier_terms = [x for x in wordlist if x not in relatedness_terms]\n",
    "outlier_terms += wordlist[0:3]\n",
    "outlier_terms += wordlist[-3:]\n",
    "outliers = [(x, int(term_freq_dict[x])) for x in outlier_terms]\n",
    "outliers_sorted = sorted(outliers, key=itemgetter(1))\n",
    "print(\"terms outlier task\")\n",
    "print(\"low freq:\")\n",
    "for n, (term, freq) in enumerate(outliers_sorted):\n",
    "    if n < 10:\n",
    "        print(str(n)+\",\", term + \",\", freq)\n",
    "    elif n==11:\n",
    "        print()\n",
    "        print(\"high freq:\")\n",
    "        print(str(n)+\",\", term + \",\", freq)\n",
    "    else:\n",
    "        print(str(n)+\",\", term + \",\", freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_models = {\"w2v\": w2v, \"hyper_quine\": hyper_quine_refined, \"n2v_consistency\": consistency}\n",
    "outlier_model_names = [key for key in outlier_models.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_name for model_name, model in outlier_models.items()]\n",
    "import itertools\n",
    "c = list(itertools.product(outlier_terms, models))\n",
    "random.shuffle(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "1\n",
      "term: conditioning    model : hyper_quine      freq : 91\n",
      "nn1: intake(0.505824625333)\n",
      "nn2: direct(0.491588300285)\n",
      "outlier: (0.18425645339784349, 'inculcated')\n",
      "-----------\n",
      "2\n",
      "term: reduction    model : n2v_consistency      freq : 217\n",
      "nn1: ('ordered_pair', 0.6487832069396973)\n",
      "nn2: ('memory', 0.5213330984115601)\n",
      "outlier: ('truths', 0.23513129353523254)\n",
      "-----------\n",
      "3\n",
      "term: pronouns    model : n2v_consistency      freq : 111\n",
      "nn1: ('about', 0.6425174474716187)\n",
      "nn2: ('divided_reference', 0.5624713897705078)\n",
      "outlier: ('desirable', 0.27970218658447266)\n",
      "-----------\n",
      "4\n",
      "term: quantification    model : hyper_quine      freq : 1671\n",
      "nn1: existential(0.602559559971)\n",
      "nn2: universal(0.586911254645)\n",
      "outlier: (0.18535683353091537, 'logic')\n",
      "-----------\n",
      "5\n",
      "term: objects    model : hyper_quine      freq : 1985\n",
      "nn1: physical(0.642044816049)\n",
      "nn2: object(0.549032999713)\n",
      "outlier: (0.21639245648127881, 'them')\n",
      "-----------\n",
      "6\n",
      "term: ambiguity    model : w2v      freq : 188\n",
      "nn1: ('instantiation', 0.7148892283439636)\n",
      "nn2: ('ôimpliesõ', 0.6782553195953369)\n",
      "outlier: ('else', 0.5875861048698425)\n",
      "-----------\n",
      "7\n",
      "term: objects    model : n2v_consistency      freq : 1985\n",
      "nn1: ('observation_sentences', 0.7791314125061035)\n",
      "nn2: ('object', 0.773756742477417)\n",
      "outlier: ('thus', 0.4062650203704834)\n",
      "-----------\n",
      "8\n",
      "term: sense_datum    model : w2v      freq : 7\n",
      "nn1: ('picks', 0.5939058065414429)\n",
      "nn2: ('dl', 0.5795704126358032)\n",
      "outlier: ('individuatives', 0.44078198075294495)\n",
      "-----------\n",
      "9\n",
      "term: dispositions    model : n2v_consistency      freq : 112\n",
      "nn1: ('physicalism', 0.7778458595275879)\n",
      "nn2: ('ambiguity', 0.7644619941711426)\n",
      "outlier: ('understand', 0.5542381405830383)\n",
      "-----------\n",
      "10\n",
      "term: truth_vehicles    model : n2v_consistency      freq : 14\n",
      "nn1: ('numbers', 0.7420178651809692)\n",
      "nn2: ('time', 0.6681967973709106)\n",
      "outlier: ('constructivism', 0.35995346307754517)\n",
      "-----------\n",
      "11\n",
      "term: ordered_pair    model : hyper_quine      freq : 191\n",
      "nn1: insidious(0.449211142399)\n",
      "nn2: ctx(0.351215558442)\n",
      "outlier: (0.20115857260535069, 'auxiliary')\n",
      "-----------\n",
      "12\n",
      "term: mentalistic    model : w2v      freq : 98\n",
      "nn1: ('contexts', 0.6314680576324463)\n",
      "nn2: ('permutations', 0.6262474656105042)\n",
      "outlier: ('times', 0.5264096260070801)\n",
      "-----------\n",
      "13\n",
      "term: nominalism    model : n2v_consistency      freq : 123\n",
      "nn1: ('parts', 0.8482801914215088)\n",
      "nn2: ('words', 0.8442181944847107)\n",
      "outlier: ('remember', 0.4742145538330078)\n",
      "-----------\n",
      "14\n",
      "term: propositional_attitudes    model : hyper_quine      freq : 138\n",
      "nn1: irremediably(0.610459825018)\n",
      "nn2: dicto(0.601306605197)\n",
      "outlier: (0.18323878258261644, 'remain')\n",
      "-----------\n",
      "15\n",
      "term: prediction    model : w2v      freq : 113\n",
      "nn1: ('names', 0.6315785646438599)\n",
      "nn2: ('intensions', 0.622445821762085)\n",
      "outlier: ('yielding', 0.5349096655845642)\n",
      "-----------\n",
      "16\n",
      "term: about    model : n2v_consistency      freq : 1408\n",
      "nn1: ('posit', 0.814532995223999)\n",
      "nn2: ('objects', 0.7275334596633911)\n",
      "outlier: ('categorical', 0.19689801335334778)\n",
      "-----------\n",
      "17\n",
      "term: reality    model : w2v      freq : 207\n",
      "nn1: ('programming', 0.7062655687332153)\n",
      "nn2: ('overlaid', 0.6881659030914307)\n",
      "outlier: ('necessity', 0.5373281836509705)\n",
      "-----------\n",
      "18\n",
      "term: about    model : w2v      freq : 1408\n",
      "nn1: ('designation', 0.694610595703125)\n",
      "nn2: ('answering', 0.6930164694786072)\n",
      "outlier: ('descriptions', 0.6094340085983276)\n",
      "-----------\n",
      "19\n",
      "term: conditionals    model : w2v      freq : 141\n",
      "nn1: ('therewith', 0.60971999168396)\n",
      "nn2: ('truth', 0.6090607047080994)\n",
      "outlier: ('geometry', 0.51873779296875)\n",
      "-----------\n",
      "20\n",
      "term: objects    model : w2v      freq : 1985\n",
      "nn1: ('generalization', 0.6695494055747986)\n",
      "nn2: ('cooperating', 0.658652663230896)\n",
      "outlier: ('there', 0.5766705274581909)\n",
      "-----------\n",
      "21\n",
      "term: intensional_abstraction    model : w2v      freq : 9\n",
      "nn1: ('wre', 0.6475462913513184)\n",
      "nn2: ('say', 0.6432003974914551)\n",
      "outlier: ('honoured', 0.548552393913269)\n",
      "-----------\n",
      "22\n",
      "term: parts    model : n2v_consistency      freq : 272\n",
      "nn1: ('ambiguity', 0.8301409482955933)\n",
      "nn2: ('truth_functions', 0.7626241445541382)\n",
      "outlier: ('even', 0.44676080346107483)\n",
      "-----------\n",
      "23\n",
      "term: mentalistic    model : hyper_quine      freq : 98\n",
      "nn1: criticisms(0.402824963789)\n",
      "nn2: vitally(0.36699841418)\n",
      "outlier: (0.17142563629807051, 'acceptable')\n",
      "-----------\n",
      "24\n",
      "term: intensional_abstraction    model : n2v_consistency      freq : 9\n",
      "nn1: ('meaning', 0.6337363123893738)\n",
      "nn2: ('translation', 0.5981030464172363)\n",
      "outlier: ('density', 0.31782227754592896)\n",
      "-----------\n",
      "25\n",
      "term: propositional_attitudes    model : n2v_consistency      freq : 138\n",
      "nn1: ('pronouns', 0.8141608834266663)\n",
      "nn2: ('posit', 0.6343877911567688)\n",
      "outlier: ('desired', 0.1842726469039917)\n",
      "-----------\n",
      "26\n",
      "term: truth_functions    model : w2v      freq : 267\n",
      "nn1: ('immanently', 0.6422076225280762)\n",
      "nn2: ('terms', 0.6384295225143433)\n",
      "outlier: ('etc', 0.5563881397247314)\n",
      "-----------\n",
      "27\n",
      "term: conditionals    model : hyper_quine      freq : 141\n",
      "nn1: categoricals(0.536312252777)\n",
      "nn2: conditional(0.526438986072)\n",
      "outlier: (0.18522357278914947, 'joint')\n",
      "-----------\n",
      "28\n",
      "term: mentalistic    model : n2v_consistency      freq : 98\n",
      "nn1: ('maxim', 0.6601607799530029)\n",
      "nn2: ('dispositions', 0.6176667213439941)\n",
      "outlier: ('memory', 0.38079312443733215)\n",
      "-----------\n",
      "29\n",
      "term: ordered_pair    model : w2v      freq : 191\n",
      "nn1: ('constitutes', 0.7229539752006531)\n",
      "nn2: ('assuring', 0.6956750154495239)\n",
      "outlier: ('ß', 0.5722965598106384)\n",
      "-----------\n",
      "30\n",
      "term: prediction    model : hyper_quine      freq : 113\n",
      "nn1: expectation(0.600630894724)\n",
      "nn2: successful(0.500053894079)\n",
      "outlier: (0.17691563816360106, 'fell')\n",
      "-----------\n",
      "31\n",
      "term: truth_vehicles    model : w2v      freq : 14\n",
      "nn1: ('pernicious', 0.5076371431350708)\n",
      "nn2: ('prolonged', 0.5026463270187378)\n",
      "outlier: ('quarks', 0.3842965066432953)\n",
      "-----------\n",
      "32\n",
      "term: conditioning    model : n2v_consistency      freq : 91\n",
      "nn1: ('reduction', 0.4902670085430145)\n",
      "nn2: ('memory', 0.423376202583313)\n",
      "outlier: ('stuff', 0.21776601672172546)\n",
      "-----------\n",
      "33\n",
      "term: truth_functions    model : hyper_quine      freq : 267\n",
      "nn1: quantification(0.479450301849)\n",
      "nn2: existential(0.408583956391)\n",
      "outlier: (0.18943116175346095, 'elementary')\n",
      "-----------\n",
      "34\n",
      "term: conditionals    model : n2v_consistency      freq : 141\n",
      "nn1: ('conditioning', 0.6936150789260864)\n",
      "nn2: ('time', 0.5768187642097473)\n",
      "outlier: ('skepticism', 0.19740596413612366)\n",
      "-----------\n",
      "35\n",
      "term: parts    model : hyper_quine      freq : 272\n",
      "nn1: spatially(0.451261865581)\n",
      "nn2: part(0.442191656073)\n",
      "outlier: (0.17709770164909933, 'space')\n",
      "-----------\n",
      "36\n",
      "term: dispositions    model : hyper_quine      freq : 112\n",
      "nn1: behaviour(0.641955548067)\n",
      "nn2: behavior(0.639929704235)\n",
      "outlier: (0.18524426972217148, 'fabric')\n",
      "-----------\n",
      "37\n",
      "term: sense_datum    model : hyper_quine      freq : 7\n",
      "nn1: flimsy(0.402284928507)\n",
      "nn2: bones(0.395628071194)\n",
      "outlier: (0.21110939537880369, 'smooth')\n",
      "-----------\n",
      "38\n",
      "term: quantification    model : n2v_consistency      freq : 1671\n",
      "nn1: ('propositional_attitudes', 0.5921144485473633)\n",
      "nn2: ('pronouns', 0.5435982942581177)\n",
      "outlier: ('ambiguity', 0.22737374901771545)\n",
      "-----------\n",
      "39\n",
      "term: reduction    model : w2v      freq : 217\n",
      "nn1: ('direct', 0.6604467034339905)\n",
      "nn2: ('dwindles', 0.6435278654098511)\n",
      "outlier: ('forms', 0.5832614898681641)\n",
      "-----------\n",
      "40\n",
      "term: about    model : hyper_quine      freq : 1408\n",
      "nn1: regarding(0.334563634022)\n",
      "nn2: world(0.325208620898)\n",
      "outlier: (0.1771007173253451, 'he')\n",
      "-----------\n",
      "41\n",
      "term: sense_datum    model : n2v_consistency      freq : 7\n",
      "nn1: ('construction', 0.5611881017684937)\n",
      "nn2: ('prediction', 0.5363653302192688)\n",
      "outlier: ('divisibility', 0.22320935130119324)\n",
      "-----------\n",
      "42\n",
      "term: ambiguity    model : hyper_quine      freq : 188\n",
      "nn1: yey(0.586461304844)\n",
      "nn2: typical(0.541681545423)\n",
      "outlier: (0.167603451659636, 'proper')\n",
      "-----------\n",
      "43\n",
      "term: dispositions    model : w2v      freq : 112\n",
      "nn1: ('absolutely', 0.5811583995819092)\n",
      "nn2: ('acceptable', 0.5772706866264343)\n",
      "outlier: ('attributed', 0.501362681388855)\n",
      "-----------\n",
      "44\n",
      "term: ordered_pair    model : n2v_consistency      freq : 191\n",
      "nn1: ('truth_vehicles', 0.6313872337341309)\n",
      "nn2: ('conditionals', 0.51842200756073)\n",
      "outlier: ('discourse', 0.2937350273132324)\n",
      "-----------\n",
      "45\n",
      "term: propositional_attitudes    model : w2v      freq : 138\n",
      "nn1: ('manifestation', 0.5634734630584717)\n",
      "nn2: ('probably', 0.5582805275917053)\n",
      "outlier: ('reached', 0.4751298427581787)\n",
      "-----------\n",
      "46\n",
      "term: reality    model : n2v_consistency      freq : 207\n",
      "nn1: ('parts', 0.6579746603965759)\n",
      "nn2: ('nouns', 0.613336443901062)\n",
      "outlier: ('reduction', 0.21448436379432678)\n",
      "-----------\n",
      "47\n",
      "term: conditioning    model : w2v      freq : 91\n",
      "nn1: ('does', 0.5304706692695618)\n",
      "nn2: ('touched', 0.5258488655090332)\n",
      "outlier: ('dispositional', 0.4167746901512146)\n",
      "-----------\n",
      "48\n",
      "term: nominalism    model : w2v      freq : 123\n",
      "nn1: ('forth', 0.518254816532135)\n",
      "nn2: ('responsive', 0.48991912603378296)\n",
      "outlier: ('fi', 0.41919559240341187)\n",
      "-----------\n",
      "49\n",
      "term: reduction    model : hyper_quine      freq : 217\n",
      "nn1: inflations(0.3472099412)\n",
      "nn2: reductions(0.32887072329)\n",
      "outlier: (0.176890555282506, 'formulation')\n",
      "-----------\n",
      "50\n",
      "term: prediction    model : n2v_consistency      freq : 113\n",
      "nn1: ('numbers', 0.690130352973938)\n",
      "nn2: ('truth_vehicles', 0.6556803584098816)\n",
      "outlier: ('metaphysics', 0.24168486893177032)\n",
      "-----------\n",
      "51\n",
      "term: parts    model : w2v      freq : 272\n",
      "nn1: ('total', 0.6527529358863831)\n",
      "nn2: ('summed', 0.6384841799736023)\n",
      "outlier: ('name', 0.48556220531463623)\n",
      "-----------\n",
      "52\n",
      "term: truth_functions    model : n2v_consistency      freq : 267\n",
      "nn1: ('variables', 0.7590671181678772)\n",
      "nn2: ('phoneme', 0.7546464204788208)\n",
      "outlier: ('application', 0.4880531430244446)\n",
      "-----------\n",
      "53\n",
      "term: intensional_abstraction    model : hyper_quine      freq : 9\n",
      "nn1: referen(0.366349076361)\n",
      "nn2: spelling(0.360269049422)\n",
      "outlier: (0.19495049767246392, 'equity')\n",
      "-----------\n",
      "54\n",
      "term: ambiguity    model : n2v_consistency      freq : 188\n",
      "nn1: ('phoneme', 0.8540822267532349)\n",
      "nn2: ('parts', 0.8301409482955933)\n",
      "outlier: ('believe', 0.5390712022781372)\n",
      "-----------\n",
      "55\n",
      "term: nominalism    model : hyper_quine      freq : 123\n",
      "nn1: disavowals(0.294504559312)\n",
      "nn2: realism(0.289672431588)\n",
      "outlier: (0.16457602676793531, 'empiricist')\n",
      "-----------\n",
      "56\n",
      "term: reality    model : hyper_quine      freq : 207\n",
      "nn1: monica(0.372022895373)\n",
      "nn2: unsullied(0.295889614144)\n",
      "outlier: (0.14953748598773581, 'circumstances')\n",
      "-----------\n",
      "57\n",
      "term: pronouns    model : hyper_quine      freq : 111\n",
      "nn1: exterior(0.544235182827)\n",
      "nn2: pronoun(0.528204785603)\n",
      "outlier: (0.16840653200293298, 'job')\n",
      "-----------\n",
      "58\n",
      "term: pronouns    model : w2v      freq : 111\n",
      "nn1: ('for', 0.6243113279342651)\n",
      "nn2: ('thatí', 0.616601288318634)\n",
      "outlier: ('tenuous', 0.5183825492858887)\n",
      "-----------\n",
      "59\n",
      "term: quantification    model : w2v      freq : 1671\n",
      "nn1: ('immanently', 0.6182039976119995)\n",
      "nn2: ('apple', 0.6098297238349915)\n",
      "outlier: ('likewise', 0.5391420722007751)\n",
      "-----------\n",
      "60\n",
      "term: truth_vehicles    model : hyper_quine      freq : 14\n",
      "nn1: surrogate(0.490350721199)\n",
      "nn2: reworking(0.416264533185)\n",
      "outlier: (0.19124140400695946, 'xiii')\n"
     ]
    }
   ],
   "source": [
    "#stimuli for the outlier detection task\n",
    "for n, (term, model) in enumerate(c):\n",
    "    freq = term_freq_dict[term]\n",
    "    print(\"-----------\")\n",
    "    print(n+ 1)\n",
    "    print(\"term: \" + term + \"    model : \" + str(model) + \"      freq : \" + str(freq))\n",
    "    get_outlier_stimuli(outlier_models[model], term)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
